{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370ab3f4-d431-46ba-bdde-efdee6adc8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import BeautifulStoneSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import tempfile\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7279fd-d667-420a-8a00-99cc18d9dd52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame()\n",
    "\n",
    "for j in range(1,50):\n",
    "    CHROMEDRIVER_PATH = r\"C:\\Users\\moham\\chrome_driver\\chromedriver-win64\\chromedriver.exe\"\n",
    "    \n",
    "    # Temp profile for fresh session\n",
    "    temp_profile = tempfile.mkdtemp()\n",
    "    \n",
    "    # Chrome options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(f\"--user-data-dir={temp_profile}\")\n",
    "    chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\")\n",
    "    \n",
    "    # Create service and driver\n",
    "    service = Service(CHROMEDRIVER_PATH)\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    \n",
    "    # Load website\n",
    "    start = time.time()\n",
    "    driver.get(\"https://www.ambitionbox.com/list-of-companies?page={}\".format(j))\n",
    "    time.sleep(5)\n",
    "    html = driver.page_source\n",
    "    driver.quit()\n",
    "    end = time.time()\n",
    "    \n",
    "    # Parse page\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    print(f\"Request took {end - start:.2f} seconds\")\n",
    "    print(\"Page Title:\", soup.title.text.strip())\n",
    "    # print(soup.prettify())\n",
    "    com_list=[]\n",
    "    for i in (soup.find_all('h2')):\n",
    "        # print(i.text.strip())\n",
    "        com_list.append(i.text.strip())\n",
    "    \n",
    "    company_series_data=pd.Series(com_list[:20])    \n",
    "    \n",
    "        # to find the company type likeits types in which it is located and what type is it \n",
    "    company_type_list=[]\n",
    "    companies_types = soup.find_all('span', class_='companyCardWrapper__interLinking')\n",
    "    \n",
    "    for company_type in companies_types:\n",
    "        # print(company_type.text.strip().replace('|','-').replace('/',','))\n",
    "        company_type_list.append(company_type.text.strip().replace('|','-').replace('/',','))\n",
    "    company_type_series=pd.Series(company_type_list)\n",
    "        # to find the rating and then converting that into the list for the dataframe \n",
    "    ratings = soup.find_all('div', style=lambda x: x and 'padding-bottom' in x)\n",
    "    rating_list=[]\n",
    "    for r in ratings:\n",
    "        rating_text = r.text.strip()\n",
    "        if rating_text.replace('.', '', 1).isdigit():  # crude check for float-like values\n",
    "            #print(rating_text)\n",
    "            rating_list.append((rating_text+\"k\"))\n",
    "    # print(rating_list)\n",
    "    rating_series=pd.Series(rating_list)\n",
    "        # Get all company cards\n",
    "    company_cards = soup.find_all('div', class_='companyCardWrapper')\n",
    "    \n",
    "    review_list = []\n",
    "    \n",
    "    for card in company_cards:\n",
    "        action_counts = card.find_all('span', class_='companyCardWrapper__ActionCount')\n",
    "        if len(action_counts) > 1:\n",
    "            review = action_counts[0].text.strip()\n",
    "            review_list.append(review)\n",
    "    \n",
    "    # Convert to Series\n",
    "    review_series = pd.Series(review_list)\n",
    "    # print(review_series)\n",
    "\n",
    "    \n",
    "        # Get all company cards\n",
    "    company_cards = soup.find_all('div', class_='companyCardWrapper')\n",
    "    \n",
    "    salary_list = []\n",
    "    \n",
    "    for card in company_cards:\n",
    "        action_counts = card.find_all('span', class_='companyCardWrapper__ActionCount')\n",
    "        if len(action_counts) > 0:\n",
    "            salary = action_counts[1].text.strip()\n",
    "            salary_list.append(salary)\n",
    "    \n",
    "    # Convert to Series\n",
    "    salary_series = pd.Series(salary_list)\n",
    "    # print(salary_series)\n",
    "    \n",
    "        # Get all company cards\n",
    "    company_cards = soup.find_all('div', class_='companyCardWrapper')\n",
    "    \n",
    "    job_list = []\n",
    "    \n",
    "    for card in company_cards:\n",
    "        action_counts = card.find_all('span', class_='companyCardWrapper__ActionCount')\n",
    "        if len(action_counts) > 3:\n",
    "            job = action_counts[3].text.strip()\n",
    "            job_list.append(job)\n",
    "    \n",
    "    # Convert to Series\n",
    "    job_series = pd.Series(job_list)\n",
    "    # print(job_series)\n",
    "    \n",
    "        # Get all company cards\n",
    "    company_cards = soup.find_all('div', class_='companyCardWrapper')\n",
    "    \n",
    "    interview_list = []\n",
    "    \n",
    "    for card in company_cards:\n",
    "        action_counts = card.find_all('span', class_='companyCardWrapper__ActionCount')\n",
    "        if len(action_counts) > 2:\n",
    "            interview = action_counts[2].text.strip()\n",
    "            interview_list.append(interview)\n",
    "    \n",
    "    # Convert to Series\n",
    "    interview_series = pd.Series(interview_list)\n",
    "    # print(interview_series)\n",
    "\n",
    "        # Get all company cards\n",
    "    company_cards = soup.find_all('div', class_='companyCardWrapper')\n",
    "    \n",
    "    highly_rated_list = []\n",
    "    \n",
    "    for card in company_cards:\n",
    "        # Check if the \"Highly Rated For\" label is present\n",
    "        high_label = card.find('span', class_='companyCardWrapper__ratingHeader--high')\n",
    "        \n",
    "        if high_label and 'Highly Rated For' in high_label.text:\n",
    "            # Extract the values next to it\n",
    "            high_values = card.find('span', class_='companyCardWrapper__ratingValues')\n",
    "            if high_values:\n",
    "                highly_rated_list.append(high_values.text.strip())\n",
    "            else:\n",
    "                highly_rated_list.append('N/A')\n",
    "        else:\n",
    "            highly_rated_list.append('N/A')\n",
    "    \n",
    "    # Convert to Series\n",
    "    highly_rated_series = pd.Series(highly_rated_list)\n",
    "    # print(highly_rated_series)\n",
    "\n",
    "        # Extracting \"Critically Rated For\" values\n",
    "    critical_list = []\n",
    "    \n",
    "    for card in company_cards:\n",
    "        # Check if the critical rating section exists (corrected class)\n",
    "        low_label = card.find('span', class_='companyCardWrapper__ratingHeader--critical')\n",
    "        \n",
    "        if low_label and 'Critically Rated For' in low_label.text:\n",
    "            # Find the associated rating values (next span)\n",
    "            low_values = low_label.find_next('span', class_='companyCardWrapper__ratingValues')\n",
    "            if low_values:\n",
    "                critical_list.append(low_values.text.strip())\n",
    "            else:\n",
    "                critical_list.append('N/A')\n",
    "        else:\n",
    "            critical_list.append('N/A')\n",
    "    \n",
    "    # Convert to Series\n",
    "    critical_series = pd.Series(critical_list)\n",
    "    # print(critical_series)\n",
    "    \n",
    "    \n",
    "    # Combine into DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Company': company_series_data,\n",
    "        'Rating': rating_series,\n",
    "        \"Reviews\":review_series,\n",
    "        \"company_type\":company_type_series,\n",
    "        'salary':salary_series,\n",
    "        \"Interview_count\": interview_series,\n",
    "        \"Jobs_available\":job_series,\n",
    "        \"highly_rated_for\":highly_rated_series,\n",
    "        \"critically_rated_for\":critical_series\n",
    "    })\n",
    "    print(df)\n",
    "    temp_df = pd.concat([temp_df, df], ignore_index=True)\n",
    "\n",
    "\n",
    "temp_df.to_csv(\"ambition_data_1000.csv\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fed548-5cfe-4cf5-9ce4-6efd6285f5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMEDRIVER_PATH = r\"C:\\Users\\moham\\Downloads\\chromedriver-win64\\chromedriver.exe\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8a5999-3f7c-4798-b68a-efbfcf3f91cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://www.ambitionbox.com/list-of-companies\"\n",
    "\n",
    "# headers = {\n",
    "#     \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\"\n",
    "# }\n",
    "\n",
    "# response = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65a9ec7-66b1-4591-ba6f-e892e90c4116",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get(\"https://www.ambitionbox.com/list-of-companies?page=1\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb889dd-6d63-41b6-b3ec-b5f7d419bc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import tempfile\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Path to chromedriver.exe\n",
    "CHROMEDRIVER_PATH = r\"C:\\Users\\moham\\chrome_driver\\chromedriver-win64\\chromedriver.exe\"\n",
    "\n",
    "# Temp profile for fresh session\n",
    "temp_profile = tempfile.mkdtemp()\n",
    "\n",
    "# Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "chrome_options.add_argument(f\"--user-data-dir={temp_profile}\")\n",
    "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\")\n",
    "\n",
    "# Create service and driver\n",
    "service = Service(CHROMEDRIVER_PATH)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Load website\n",
    "start = time.time()\n",
    "driver.get(\"https://www.ambitionbox.com/list-of-companies?page=1\")\n",
    "time.sleep(5)\n",
    "html = driver.page_source\n",
    "driver.quit()\n",
    "end = time.time()\n",
    "\n",
    "# Parse page\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "print(f\"Request took {end - start:.2f} seconds\")\n",
    "print(\"Page Title:\", soup.title.text.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11768c6-ea40-48d8-93df-e4ffdcef1457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc401d30-38b1-4543-bc33-ac1df070413a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f609dea-44b2-4b63-95ad-c6ccbd766e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('h1')[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c102a5f-e942-4552-957b-1d85878e585b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "com_list=[]\n",
    "for i in (soup.find_all('h2')):\n",
    "    print(i.text.strip())\n",
    "    com_list.append(i.text.strip())\n",
    "\n",
    "company_series_data=pd.Series(com_list[:20])    \n",
    "company_series_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a15f65-d8a8-407c-8749-4ae0c22bb781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find the company type likeits types in which it is located and what type is it \n",
    "company_type_list=[]\n",
    "companies_types = soup.find_all('span', class_='companyCardWrapper__interLinking')\n",
    "\n",
    "for company_type in companies_types:\n",
    "    print(company_type.text.strip().replace('|','-').replace('/',','))\n",
    "    company_type_list.append(company_type.text.strip().replace('|','-').replace('/',','))\n",
    "company_type_series=pd.Series(company_type_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0c7b3f-72ec-47d6-8fa3-08de432038f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find the rating and then converting that into the list for the dataframe \n",
    "ratings = soup.find_all('div', style=lambda x: x and 'padding-bottom' in x)\n",
    "rating_list=[]\n",
    "for r in ratings:\n",
    "    rating_text = r.text.strip()\n",
    "    if rating_text.replace('.', '', 1).isdigit():  # crude check for float-like values\n",
    "        #print(rating_text)\n",
    "        rating_list.append((rating_text+\"k\"))\n",
    "print(rating_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cdbc4e-98ad-427a-b4f6-f5a721c8dd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all company cards\n",
    "company_cards = soup.find_all('div', class_='companyCardWrapper')\n",
    "\n",
    "review_list = []\n",
    "\n",
    "for card in company_cards:\n",
    "    action_counts = card.find_all('span', class_='companyCardWrapper__ActionCount')\n",
    "    if len(action_counts) > 1:\n",
    "        review = action_counts[0].text.strip()\n",
    "        review_list.append(review)\n",
    "\n",
    "# Convert to Series\n",
    "review_series = pd.Series(review_list)\n",
    "print(review_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5458aac7-d89b-4944-ac8f-0cecc33e4ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all company cards\n",
    "company_cards = soup.find_all('div', class_='companyCardWrapper')\n",
    "\n",
    "salary_list = []\n",
    "\n",
    "for card in company_cards:\n",
    "    action_counts = card.find_all('span', class_='companyCardWrapper__ActionCount')\n",
    "    if len(action_counts) > 0:\n",
    "        salary = action_counts[1].text.strip()\n",
    "        salary_list.append(salary)\n",
    "\n",
    "# Convert to Series\n",
    "salary_series = pd.Series(salary_list)\n",
    "print(salary_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb74834-0631-49e6-b284-197cc7ad44d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all company cards\n",
    "company_cards = soup.find_all('div', class_='companyCardWrapper')\n",
    "\n",
    "job_list = []\n",
    "\n",
    "for card in company_cards:\n",
    "    action_counts = card.find_all('span', class_='companyCardWrapper__ActionCount')\n",
    "    if len(action_counts) > 3:\n",
    "        job = action_counts[3].text.strip()\n",
    "        job_list.append(job)\n",
    "\n",
    "# Convert to Series\n",
    "job_series = pd.Series(job_list)\n",
    "print(job_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b7b440-692f-48bd-86f0-7ff576b9c321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all company cards\n",
    "company_cards = soup.find_all('div', class_='companyCardWrapper')\n",
    "\n",
    "interview_list = []\n",
    "\n",
    "for card in company_cards:\n",
    "    action_counts = card.find_all('span', class_='companyCardWrapper__ActionCount')\n",
    "    if len(action_counts) > 2:\n",
    "        interview = action_counts[2].text.strip()\n",
    "        interview_list.append(interview)\n",
    "\n",
    "# Convert to Series\n",
    "interview_series = pd.Series(interview_list)\n",
    "print(interview_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7adc52-a88e-4e86-84a1-378db2303d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all company cards\n",
    "company_cards = soup.find_all('div', class_='companyCardWrapper')\n",
    "\n",
    "highly_rated_list = []\n",
    "\n",
    "for card in company_cards:\n",
    "    # Check if the \"Highly Rated For\" label is present\n",
    "    high_label = card.find('span', class_='companyCardWrapper__ratingHeader--high')\n",
    "    \n",
    "    if high_label and 'Highly Rated For' in high_label.text:\n",
    "        # Extract the values next to it\n",
    "        high_values = card.find('span', class_='companyCardWrapper__ratingValues')\n",
    "        if high_values:\n",
    "            highly_rated_list.append(high_values.text.strip())\n",
    "        else:\n",
    "            highly_rated_list.append('N/A')\n",
    "    else:\n",
    "        highly_rated_list.append('N/A')\n",
    "\n",
    "# Convert to Series\n",
    "highly_rated_series = pd.Series(highly_rated_list)\n",
    "print(highly_rated_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd24848-fb04-49a4-b216-b487ec5c9e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting \"Critically Rated For\" values\n",
    "critical_list = []\n",
    "\n",
    "for card in company_cards:\n",
    "    # Check if the critical rating section exists (corrected class)\n",
    "    low_label = card.find('span', class_='companyCardWrapper__ratingHeader--critical')\n",
    "    \n",
    "    if low_label and 'Critically Rated For' in low_label.text:\n",
    "        # Find the associated rating values (next span)\n",
    "        low_values = low_label.find_next('span', class_='companyCardWrapper__ratingValues')\n",
    "        if low_values:\n",
    "            critical_list.append(low_values.text.strip())\n",
    "        else:\n",
    "            critical_list.append('N/A')\n",
    "    else:\n",
    "        critical_list.append('N/A')\n",
    "\n",
    "# Convert to Series\n",
    "critical_series = pd.Series(critical_list)\n",
    "print(critical_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf93d1e8-b804-4562-903a-3e30647bdf5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "series_data = pd.Series(com_list[:20])\n",
    "rating_data = pd.Series(rating_list[:20])\n",
    "\n",
    "# Combine into DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Company': series_data,\n",
    "    'Rating': rating_data,\n",
    "    \"Reviews\":review_series,\n",
    "    \"company_type\":company_type_series,\n",
    "    'salary':salary_series,\n",
    "    \"Interview_count\":interviews_series,\n",
    "    \"Jobs_available\":job_series,\n",
    "    \"highly_rated_for\":highly_rated_series,\n",
    "    \"critically_rated_for\":critical_series\n",
    "})\n",
    "\n",
    "print(df)\n",
    "df.to_csv('Ambition_box_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc15d2e-2122-400f-80bb-bbd8566b2e02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this code is for fetching the reviews from the website and then converting them into the list for the dataframe \n",
    "import re\n",
    "\n",
    "reviews_list=[]\n",
    "rating_counts = soup.find_all('span', class_='companyCardWrapper__companyRatingCount')\n",
    "\n",
    "for rating in rating_counts:\n",
    "    print(rating.text.strip())\n",
    "    reviews_list.append(rating.text.strip())\n",
    "\n",
    "cleaned_reviews=[r.strip('()') for r in reviews_list]\n",
    "\n",
    "review_series=pd.Series(cleaned_reviews)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e03cd9d-27a7-4743-a340-3a9c4e733ecc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get all company cards\n",
    "company_cards = soup.find_all('div', class_='companyCardWrapper')\n",
    "\n",
    "salaries_list = []\n",
    "\n",
    "for card in company_cards:\n",
    "    action_counts = card.find_all('span', class_='companyCardWrapper__ActionCount')\n",
    "    if action_counts:\n",
    "        salary = action_counts[1].text.strip()  # First item is Salary\n",
    "        salaries_list.append(salary)\n",
    "\n",
    "# Convert to Series\n",
    "salary_series = pd.Series(salaries_list)\n",
    "print(salary_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eeff55-ec06-47bf-95f5-729197837aee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f133da-53b0-4ef1-aa2c-282bf8cfaa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all company cards\n",
    "company_cards = soup.find_all('div', class_='companyCardWrapper')\n",
    "\n",
    "job_list = []\n",
    "\n",
    "for card in company_cards:\n",
    "    action_counts = card.find_all('span', class_='companyCardWrapper__ActionCount')\n",
    "    if action_counts:\n",
    "        job = action_counts[3].text.strip()  # First item is Salary\n",
    "        job_list.append(job)\n",
    "\n",
    "# Convert to Series\n",
    "job_series = pd.Series(job_list)\n",
    "print(job_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206fde02-debc-4452-88ad-f0a1e53627d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to fetch the salaries for the companies and then converting them into a series\n",
    "salaries_list=[]\n",
    "salaries_values=soup.find_all('span',class_='companyCardWrapper__ActionCount')\n",
    "for  i in salaries_values:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f603be7-be77-41d5-b5c0-7dc21ffee050",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b0bd3c-a756-4c68-9164-8f092bc97e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('Ambition_box_data.csv')[['Company','critically_rated_for']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c902a6f-e41f-4a8f-bc1c-8666d7cd0ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.read_csv('Final_ambition_data.csv')\n",
    "df2 = df2.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4819e112-b859-4082-9b5e-e67488530761",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('Final_ambition_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ac2269-7e8b-4eee-90b0-df1170a55001",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.read_csv('Final_ambition_data.csv')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc72f5e-1fd7-45c6-99dd-24f8e681adac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
